// Copyright 2012-2014 Tetsuo Kiso. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package pegasos

import (
	"testing"
)

type tokenizeTest struct {
	in string
	id int
	v  float64
}

var tokenizeTests = []tokenizeTest{
	{"1:3.0", 1, 3.0},
	{"2:1.0", 2, 1.0},
	{"4:3.0", 4, 3.0},
}

func setupExpectedFeatureVector() *FeatureVector {
	fv := NewFeatureVector(3)
	nodes := []Node{{1, 3.0}, {2, 1.0}, {4, 3.0}}
	for _, v := range nodes {
		fv.PushBack(v)
	}
	return fv
}

func TestTokenizeNode(t *testing.T) {
	for _, test := range tokenizeTests {
		if v, _ := tokenizeNode(test.in); test.id != v.id || !close(test.v, v.v) {
			t.Errorf("%v, want {%d, %g}", v, test.id, test.v)
		}
	}
}

func TestTokenize(t *testing.T) {
	testStr := "+1 1:3.0 2:1.0 4:3.0"

	expected := Example{setupExpectedFeatureVector(), 1}
	expectedId := 4
	e, id, _ := Tokenize(testStr)

	if id != expectedId {
		t.Errorf("Invalid id %v, want %v", id, expectedId)
	}

	if !e.Equal(expected) {
		t.Errorf("%v, want %v", e, expected)
	}
}

func BenchmarkTokenizeNode(b *testing.B) {
	benchmarkStr := "1:99.0"
	for i := 0; i < b.N; i++ {
		tokenizeNode(benchmarkStr)
	}
}

func BenchmarkTokenize(b *testing.B) {
	// A typical feature vector representaiton used in real NLP.
	s := "+1 6:0.0198403253586671 15:0.0339873732306071 29:0.0360280968798065 31:0.0378103484117687 41:0.0456787263779904 63:0.021442413608662 74:0.0813238108919922 75:0.0201048944012214 81:0.0603996615380116 14 2:0.0102897706466067 172:0.0777948548082322 174:0.072717200608936 179:0.054076743737027 180:0.0764456665578607 186:0.112586705083256 187:0.0925011727805475 205:0.156990770998115 229:0.0519505660963924 255:0.0923321093879611 308:0.0732972342194965 318:0.119740882706743 408:0.058414185072804 409:0.0506626198519805 465:0.0843545829662396 480:0.0729642744872502 519:0.118611296605205 664:0.112142083701314 679:0.374387819227881 720:0.0987664035972632 768:0.123975200617516 922:0.141018083523918 977:0.136393581474495 1018:0.107648758381437 1305:0.180449632267364 1581:0.141526866911118 1677:0.156124608446181 1817:0.141018083523918 2162:0.170921341813635 2314:0.164249324532582 2358:0.508349039100422 2419:0.150582824316425 3266:0.338899359400281 3374:0.166725496161846 8311:0.219691455487068 1 6:0.0292418053787394 11:0.0438009834096617 15:0.0500925330294462 26:0.0210944325344804 27:0.0141908540227881 31:0.0371513682217675 63:0.0316030546001617 67:0.0275682881016999 75:0.0197544949024337 81:0.0593469819910562 92:0.0321731851678454 94:0.0292573442413133 97:0.0304107371257703 142:0.00631902168102332 153:0.0422702792713471 173:0.0364278046265029 174:0.142899688014398 176:0.101048732113733 185:0.0283803505729403 198:0.0424355763603897"
	for i := 0; i < b.N; i++ {
		Tokenize(s)
	}
}
